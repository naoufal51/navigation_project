import numpy as np
import random
from collections import namedtuple, deque

from model import QNetwork, DuelingQNetwork

import torch
import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn


from typing import Tuple

# Initialize DQN parameters
BUFFER_SIZE = 10000  # replay buffer size
BATCH_SIZE = 64  # minibatch size taken from replay buffer
GAMMA = 0.99  # discount factor
LEARNING_RATE = 1e-3  # learning rate
UPDATE_EVERY = 4  # how often to update the target network
TAU = 1e-3  # for soft update of target parameters

# define the device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class ReplayBuffer:
    """
    Buffer for storing and sampling experiences generated by the agents.

    Attributes:
        memory (deque): A deque object for storing experiences.
        batch_size (int): Size of the sampled experience batch.
        experience (namedtuple): A named tuple for storing the experience.
        seed (int): Random seed.

    Methods:
        add: Add new experience to memory.
        sample: Randomly sample a batch of experiences from the replay buffer.
        __len__: Return the length of the replay buffer.
    """

    def __init__(self, buffer_size, batch_size, seed):
        """
        Initialize the replay buffer object.

        Args:
            buffer_size (int): Size of the Replay buffer.
            batch_size (int): Size of the sampled experience batch.
            seed (int): Random seed.

        Returns:
            None.
        """
        # Rolling buffer for holding the experiences
        self.memory = deque(maxlen=buffer_size)
        self.batch_size = batch_size
        # Define a name tuple for structuting the experiences
        self.experience = namedtuple(
            "Experience",
            field_names=["state", "action", "reward", "next_state", "done"],
        )
        self.seed = random.seed(seed)

    def add(self, state, action, reward, next_state, done):
        """
        Add new experience to memory

        Args:
            state (numpy.ndarray): State vector containing the state of the agent (observation).
            action (int): Action taken by the agent after observing the state of the environment.
            reward (float): Reward given to the agent after acting.
            next_state (numpy.ndarray): Next State vector containing the Next state of the agent after acting.
            done (bool):  Indicates whether the episode is over.

        Returns:
            None.

        """
        exp = self.experience(state, action, reward, next_state, done)
        self.memory.append(exp)

    def sample(self) -> Tuple:
        """
        Randomly sample a batch of experiences from the replay buffer.

        Args:
            None.

        Returns:
            samples (Tuple): A tuple containing the sampled experiences.

        """
        experiences = random.sample(self.memory, k=self.batch_size)

        # Unpack the name tuple
        states, actions, rewards, next_states, dones = zip(*experiences)
        states = torch.from_numpy(np.vstack(states)).float().to(device)
        actions = torch.from_numpy(np.vstack(actions)).long().to(device)
        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)
        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)
        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)

        samples = (states, actions, rewards, next_states, dones)
        return samples

    def __len__(self):
        """Return the number of experience elements currently stored in memory."""
        return len(self.memory)


class DQNAgent:
    """
    Deep Q-Network agent that combines deep learning with Q-learning.
    The main objective of DQN is to estimate the Q-values, which are the expected future rewards for each action and each state.
    DQN has the following main components:
        1. Q-Network: A neural network that approximates the Q-values for a given state-action pair.
        2. Target-Network: A copy of the Q-Network that is updated every UPDATE_EVERY steps.
        3. Replay-Buffer: A buffer for storing experiences generated by the agent.
        4. Exploration-Strategy: Epsilon-Greedy exploration strategy. The agent occosiannly takes a randoma action.
    We include Dueling DQN.
    Dueling DQN:
        The Dueling DQN modifies the Q-Network to account for the advantage of the state.
        We decompose the Q-value in two parts:
            1. Value: The value of the state.
            2. Advantage: The advantage of taking the action given the state.
        This decomposition allows for better training stability and faster convergence.

    Attributes:
        state_size (int): Dimension of the state vector.
        action_size (int): Dimension of the action vector.
        gamma (float): Discount factor.
        update_every (int): Update the Target-Network every UPDATE_EVERY steps.
        time_step (int): Time step.
        device (torch.device): Device to run the model.
        seed (int): Random seed.
        learning_rate (float): Learning rate.
        q_network (QNetwork): Q-Network.
        target_q_network (QNetwork): Target-Network.
        optimizer (optim.Adam): Optimizer.
        memory (ReplayBuffer): ReplayBuffer.

    Methods:
        step: Add Agent experiences to memory and learn from experiences given some time conditions.
        act: Epsilon-Greedy action selection given the current state an policy.
        learn: Update the Q-Network and Target-Network.
        soft_update: Update the Target-Network.

    """

    def __init__(
        self,
        state_size: int,
        action_size: int,
        fc1_dim: int,
        fc2_dim: int,
        seed: int,
        dueling: bool,
    ):
        """
        Initialize the DQN agent.

        Args:
            state_size (int): Dimension of the state vector.
            action_size (int): Dimension of the action vector.
            fc1_dim (int): Number of neurons in the first fully-connected layer.
            fc2_dim (int): Number of neurons in the second fully-connected layer.
            seed (int): Random seed.

        Returns:
            None
        """
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = GAMMA
        self.update_every = UPDATE_EVERY
        # init time step
        self.time_step = 0
        # self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.seed = seed
        self.learning_rate = LEARNING_RATE

        # Q-Network
        if dueling:
            self.q_network = DuelingQNetwork(
                state_size, action_size, fc1_dim, fc2_dim
            ).to(device)
            self.target_q_network = DuelingQNetwork(
                state_size, action_size, fc1_dim, fc2_dim
            ).to(device)

        else:
            self.q_network = QNetwork(state_size, action_size, fc1_dim, fc2_dim).to(
                device
            )
            self.target_q_network = QNetwork(
                state_size, action_size, fc1_dim, fc2_dim
            ).to(device)

        # Initialize target network
        self.target_q_network.load_state_dict(self.q_network.state_dict())

        # Initialize optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)

        # Initialize the replay buffer
        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)

    def step(
        self,
        state: np.ndarray,
        action: int,
        reward: float,
        next_state: np.ndarray,
        done: bool,
    ):
        """
        Add Agent experiences to memory and learn from experiences given some time conditions.

        Args:
            state (np.ndarray): State vector containing the state of the agent (observation).
            action (int): Action taken by the agent after observing the state of the environment.
            reward (float): Reward given to the agent after acting.
            next_state (np.ndarray): Next State vector containing the Next state of the agent after acting.
            done (bool):  Indicates whether the episode is over.

        Returns:
            None.

        """

        # Save experience in replay buffer
        self.memory.add(state, action, reward, next_state, done)
        self.time_step = (self.time_step + 1) % UPDATE_EVERY

        # Learn for UPDATE_EVERY time steps
        if self.time_step == 0 and len(self.memory) > BATCH_SIZE:
            experiences = self.memory.sample()
            self.learn(experiences, self.gamma)

    def act(self, state: np.ndarray, eps: float = 0.0) -> int:
        """
        Generates actions given the current state and policy.

        Args:
            state (np.ndarray): State vector containing the state of the agent (observation).
            eps (float): Value for epsilon-greedy action strategy.

        Returns:
            action (int): Selected action
        """
        if random.random() <= eps:
            action = random.choice(np.arange(self.action_size))
        else:
            state = torch.from_numpy(state).float().unsqueeze(0).to(device)
            self.q_network.eval()
            with torch.no_grad():
                action_values = self.q_network(state)
            self.q_network.train()
            action = np.argmax(action_values.cpu().data.numpy())
        return action

    def learn(self, experiences: Tuple[torch.Tensor], gamma: float):
        """
        Learn from generated agent experiences.
        We update the model paramters given a user-specified frequency.

        Args:
            experiences (Tuple): Tuple holding the sampled experiences.
            gamma (float): Discount Factor. Used to compute the expected future reward.
        """
        states, actions, rewards, next_states, dones = experiences

        # Compute the expected Q values from the main q-network
        Q_expected = self.q_network(states).gather(1, actions)

        # Compute the expected Q values from the target q-network
        with torch.no_grad():
            Q_targets_next = (
                self.target_q_network(next_states).detach().max(1)[0].unsqueeze(1)
            )
        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))

        # Compute the loss
        loss = F.mse_loss(Q_expected, Q_targets)

        # Optimize the model
        self.optimizer.zero_grad()  # clear the gradient
        loss.backward()  # backpropagation
        self.optimizer.step()  # update the parameters

        # Update the target network
        self.soft_update(self.q_network, self.target_q_network, TAU)

    def soft_update(self, network: nn.Module, target_network: nn.Module, tau: float):
        """
        Soft update of the target network.
        theta_target = tau*theta + (1 - tau)*theta_target

        Args:
            network (nn.Module): Main network.
            target_network (nn.Module): Target network.
            tau (float): Value for the soft update. Used to control the interpolation.

        Returns:
            None
        """
        for target_param, param in zip(
            target_network.parameters(), network.parameters()
        ):
            target_param.data.copy_(param.data * tau + target_param.data * (1.0 - tau))


class DoubleDQNAgent(DQNAgent):
    """
    The Double DQN is used to reduce the issue of Q-value overestimation.
    It uses the main Q-Network to get the action and the target Q-Network to get the expected future reward.

    Dueling DQN:
        The Dueling DQN modifies the Q-Network to account for the advantage of the state.
        We decompose the Q-value in two parts:
            1. Value: The value of the state.
            2. Advantage: The advantage of taking the action given the state.
        This decomposition allows for better training stability and faster convergence.


    """

    def learn(self, experiences: Tuple[torch.Tensor], gamma: float):
        """
        Learn from generated agent experiences using the Double DQN approach.

        Args:
            experiences (Tuple): Tuple holding the sampled experiences.
            gamma (float): Discount Factor. Used to compute the expected future reward.
        """
        states, actions, rewards, next_states, dones = experiences

        # Compute the expected Q values from the main q-network
        Q_expected = self.q_network(states).gather(1, actions)

        # Compute the expected Q values using the Double DQN approach
        with torch.no_grad():
            # Find the best action using the online (regular) network
            best_actions = self.q_network(next_states).argmax(1).unsqueeze(1)

            # Use these actions to find the Q-value from the target network
            Q_targets_next = self.target_q_network(next_states).gather(1, best_actions)

        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))

        # Compute the loss
        loss = F.mse_loss(Q_expected, Q_targets)

        # Optimize the model
        self.optimizer.zero_grad()  # clear the gradient
        loss.backward()  # backpropagation
        self.optimizer.step()  # update the parameters

        # Update the target network
        self.soft_update(self.q_network, self.target_q_network, TAU)
